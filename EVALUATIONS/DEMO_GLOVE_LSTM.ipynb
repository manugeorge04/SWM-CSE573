{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "MkO-GOp9ck9t",
    "outputId": "0f8b43ad-3e4c-413f-be69-505bfe990c72",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Requirement already satisfied: nlppreprocess in /usr/local/lib/python3.6/dist-packages (1.0.2)\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "!pip install nlppreprocess\n",
    "from nlppreprocess import NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "nlp = NLP(remove_punctuations=False)\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download(\"popular\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, Bidirectional,LSTM, Embedding, Dropout,Conv1D, MaxPooling1D, Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "wMZdtcmcwcsc",
    "outputId": "722e49a1-0d23-48bc-df9d-5d09fd889656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "As6hGglmck9x"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.stem import LancasterStemmer,PorterStemmer\n",
    "\n",
    "def split_document_to_chunks(row, field_name, chunk_len):\n",
    "    text = row[field_name]\n",
    "    if(text!='[]'):\n",
    "        label = row['label']\n",
    "        sentences = sent_tokenize(text)\n",
    "        output = []\n",
    "        for i in range(0,len(sentences), chunk_len):\n",
    "            if(i+chunk_len < len(sentences)):\n",
    "                chunk = ''.join(sentences[i:i+chunk_len])\n",
    "            else:\n",
    "                chunk = ''.join(sentences[i:len(sentences)])\n",
    "            \n",
    "            num_words = len(chunk.split(' '))\n",
    "            if(num_words>2 and num_words<400):\n",
    "                output.append((chunk,label))\n",
    "        \n",
    "        if(len(output)==0):\n",
    "            return None\n",
    "        return output\n",
    "\n",
    "def prepare_data_set(data_set,field_name, chunk_size=1):\n",
    "    chunked_text_labels = data_set.apply(split_document_to_chunks, args=(field_name, chunk_size), axis=1)\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for chunks in chunked_text_labels:\n",
    "        if(chunks is not None):\n",
    "            for chunk in chunks:\n",
    "                X.append(chunk[0])\n",
    "                y.append(chunk[1])\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def nonAsciiChar(words):\n",
    "    final_words=[]\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        word=re.sub(\"\\d+\", \"\",word)\n",
    "        word=re.sub('[^a-zA-Z]+','',word)\n",
    "        f_word=unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        final_words.append(f_word)\n",
    "        \n",
    "    return final_words\n",
    "\n",
    "def removeSpace(words):\n",
    "    final_words=[]\n",
    "    for word in words:\n",
    "        if word!='':\n",
    "            final_words.append(word)\n",
    "    return final_words\n",
    "\n",
    "def stem_words(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    final_stem=[]\n",
    "    for word in words:\n",
    "        word=stemmer.stem(word)\n",
    "        if word not in final_stem:\n",
    "            final_stem.append(word)\n",
    "    return final_stem\n",
    "\n",
    "def remove_links(words):\n",
    "    final_words=[]\n",
    "    for word in words:\n",
    "        if not re.match('[www]',word):\n",
    "            final_words.append(word)\n",
    "    return final_words\n",
    "\n",
    "def clean_data(text):\n",
    "    if(text != \"[]\"):\n",
    "        sentences_list = sent_tokenize(text)\n",
    "        sentences_processed = []\n",
    "        for sentence in sentences_list:\n",
    "            processed_sentence = nlp.process(sentence)\n",
    "            words=word_tokenize(processed_sentence)\n",
    "            words=nonAsciiChar(words)\n",
    "            words=removeSpace(words)\n",
    "            #words=stem_words(words)\n",
    "            words=remove_links(words)\n",
    "            sentences_processed.append(' '.join(word for word in words))\n",
    "        return \"\".join(sentences_processed)\n",
    "    else: \n",
    "        return text\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print(' val_loss: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsQ437rack90"
   },
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('/content/drive/My Drive/SWM/final_dataset_joined_aapl60_onlyMentions.csv')\n",
    "data_set.loc[data_set['label']==-1,'label'] = 0\n",
    "data_set['filteredtext_aapl'] = data_set['filteredtext_aapl'].apply(clean_data)\n",
    "data_set = data_set.drop_duplicates(subset=['filteredtext_aapl'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "G0lBtvRCy5bA",
    "outputId": "5d70282c-e139-4ec3-e422-3590758edcc8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>news_file_loc</th>\n",
       "      <th>filteredtext_aapl</th>\n",
       "      <th>filteredtext_amzn</th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>../Data/News/2018-01/news_0000265.json</td>\n",
       "      <td>nape summit also feature annual nape summit ch...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-07 20:30:00</td>\n",
       "      <td>169.61</td>\n",
       "      <td>169.77</td>\n",
       "      <td>169.27</td>\n",
       "      <td>169.29</td>\n",
       "      <td>4552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>../Data/News/2018-01/news_0001827.json</td>\n",
       "      <td>from apples hugely anticipated iphone x samsun...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11 13:30:00</td>\n",
       "      <td>169.39</td>\n",
       "      <td>169.50</td>\n",
       "      <td>169.01</td>\n",
       "      <td>169.13</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>../Data/News/2018-01/news_0000282.json</td>\n",
       "      <td>they are not just largest browser but largest ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-12 13:30:00</td>\n",
       "      <td>172.40</td>\n",
       "      <td>172.49</td>\n",
       "      <td>172.02</td>\n",
       "      <td>172.10</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>../Data/News/2018-01/news_0001849.json</td>\n",
       "      <td>humanitarian crisis in drc placed technology p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13 13:30:00</td>\n",
       "      <td>171.62</td>\n",
       "      <td>172.62</td>\n",
       "      <td>171.62</td>\n",
       "      <td>172.55</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>../Data/News/2018-01/news_0001855.json</td>\n",
       "      <td>cramer prefers finisar nasdaq fnsr after apple...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-14 13:30:00</td>\n",
       "      <td>172.41</td>\n",
       "      <td>172.56</td>\n",
       "      <td>172.24</td>\n",
       "      <td>172.32</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                site                           news_file_loc  ... volume label\n",
       "0   businesswire.com  ../Data/News/2018-01/news_0000265.json  ...   4552     0\n",
       "1          yahoo.com  ../Data/News/2018-01/news_0001827.json  ...    229     0\n",
       "2          yahoo.com  ../Data/News/2018-01/news_0000282.json  ...    274     0\n",
       "3  investingnews.com  ../Data/News/2018-01/news_0001849.json  ...    269     1\n",
       "4   seekingalpha.com  ../Data/News/2018-01/news_0001855.json  ...    141     1\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('content/drive/My Drive/SWM/train_aapl_60.csv') \n",
    "test_df = pd.read_csv('content/drive/My Drive/SWM/test_aapl_60.csv') \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "FNk5DSdHck93",
    "outputId": "8ff55ee3-086d-4d70-ebc8-8d39c98920fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape :  43648 43648\n",
      "Test Shape :  9890 9890\n",
      "0    nape summit also feature annual nape summit ch...\n",
      "1    from apples hugely anticipated iphone x samsun...\n",
      "2    they are not just largest browser but largest ...\n",
      "3    humanitarian crisis in drc placed technology p...\n",
      "dtype: object\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "dtype: int64\n",
      "1    22471\n",
      "0    21177\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train_text,y_train_labels = prepare_data_set(train_df,'filteredtext_aapl')\n",
    "X_test_text,y_test_labels = prepare_data_set(test_df,'filteredtext_aapl')\n",
    "print(\"Train Shape : \",len(X_train_text),len(y_train_labels))\n",
    "print(\"Test Shape : \",len(X_test_text),len(y_test_labels))\n",
    "print(pd.Series(X_train_text[0:4]))\n",
    "print(pd.Series(y_train_labels[0:4]))\n",
    "print(pd.Series(y_train_labels).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGoVrIYMzepR"
   },
   "source": [
    "TOKENIZATION AND SEQUENCE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "7LLMmnMfck9-",
    "outputId": "59d5148d-5c9b-4f33-b5ac-c7ed892fad0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 127792 unique tokens.\n",
      "Shape of data tensor: (53538, 150)\n",
      "Shape of label tensor: (53538, 2)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0 20418  2836    37   676   592 20418  2836 48991 38096   654\n",
      " 48992  2347   974    20  1651  2283 19093  5501  3408 31438  3207     8\n",
      "   550  5645 31439 24228 24229     3     4  3119     3   690 20418 27188\n",
      "  1747   256   677  6062  7205     3    46     3   218  2740 31440     3\n",
      "  5756 19094   440 12087 38097  1074]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NUM_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "X_total = X_train_text+ X_test_text\n",
    "y_total = y_train_labels+ y_test_labels\n",
    "\n",
    "print('Processing text dataset')\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_total)\n",
    "sequences = tokenizer.texts_to_sequences(X_total)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = np.asarray(y_total)\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "labels = to_categorical(np.asarray(y_total))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(data[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06jmx8a-z2QU"
   },
   "source": [
    "SEQUENCE LENGTH STATS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "UyytpDglZM_5",
    "outputId": "f2993180-96cf-4c82-9e55-b14e58fdcf70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    53538.000000\n",
       "mean       118.820464\n",
       "std        108.865573\n",
       "min          1.000000\n",
       "25%         30.000000\n",
       "50%         70.000000\n",
       "75%        193.000000\n",
       "max        399.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [ len(x) for x in sequences]\n",
    "pd.Series(lens).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "fi_5eEvVck-B",
    "outputId": "072944d3-f4b1-4f41-a8e2-a9ae4eb97e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data tensor: (43648, 150)\n",
      "Shape of train label tensor: (43648, 2)\n",
      "Shape of test data tensor: (9890, 150)\n",
      "Shape of test label tensor: (9890, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:len(X_train_text)]\n",
    "y_train = labels[:len(y_train_labels)]\n",
    "X_test = data[len(X_train_text):]\n",
    "y_test = labels[len(y_train_labels):]\n",
    "print('Shape of train data tensor:', X_train.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "print('Shape of test data tensor:', X_test.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OiWdwIMb3q0j"
   },
   "source": [
    "GLOVE EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "W74ZG1njck-F",
    "outputId": "62d4d8d9-5522-4c5c-cfeb-b3a9b71a262c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Preparing embedding matrix.\n",
      "[-0.071953    0.23127     0.023731   -0.50638002  0.33923     0.19589999\n",
      " -0.32943001  0.18364    -0.18057001  0.28963     0.20448001 -0.54960001\n",
      "  0.27399001  0.58327001  0.20468    -0.49228001  0.19973999 -0.070237\n",
      " -0.88049001  0.29484999  0.14071    -0.1009      0.99449003  0.36973\n",
      "  0.44554001  0.28997999 -0.1376     -0.56365001 -0.029365   -0.4122\n",
      " -0.25268999  0.63181001 -0.44767001  0.24363001 -0.10813     0.25163999\n",
      "  0.46967     0.37549999 -0.23613    -0.14128999 -0.44536999 -0.65736997\n",
      " -0.042421   -0.28636    -0.28810999  0.063766    0.20281    -0.53542\n",
      "  0.41306999 -0.59722    -0.38613999  0.19389001 -0.17809001  1.66180003\n",
      " -0.011819   -2.3736999   0.058427   -0.26980001  1.2823      0.81924999\n",
      " -0.22322001  0.72931999 -0.053211    0.43507001  0.85010999 -0.42934999\n",
      "  0.92663997  0.39050999  1.05850005 -0.24561    -0.18265    -0.53280002\n",
      "  0.059518   -0.66018999  0.18990999  0.28836    -0.24339999  0.52784002\n",
      " -0.65762001 -0.14081     1.04910004  0.51340002 -0.23816     0.69894999\n",
      " -1.4813     -0.24869999 -0.17936    -0.059137   -0.08056    -0.48782\n",
      "  0.014487   -0.62589997 -0.32367     0.41861999 -1.08070004  0.46742001\n",
      " -0.49930999 -0.71894997  0.86894     0.19539   ]\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, '')\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, '/content/drive/My Drive/SWM/glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XVVyjqJN30EL"
   },
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "DyhjWAAtck-I",
    "outputId": "af37c181-3bb9-460e-da06-91e819fc384e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.5060 val_loss: 0.6918821930885315\n",
      "341/341 [==============================] - 11s 32ms/step - loss: 0.6970 - accuracy: 0.5060\n",
      "Epoch 2/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.5256 val_loss: 0.6903800964355469\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.6920 - accuracy: 0.5256\n",
      "Epoch 3/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.5321 val_loss: 0.6900606155395508\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.6899 - accuracy: 0.5321\n",
      "Epoch 4/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6864 - accuracy: 0.5455 val_loss: 0.6837099194526672\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.6864 - accuracy: 0.5455\n",
      "Epoch 5/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6823 - accuracy: 0.5546 val_loss: 0.6805697679519653\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.6823 - accuracy: 0.5546\n",
      "Epoch 6/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.5741 val_loss: 0.6613954901695251\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.6715 - accuracy: 0.5741\n",
      "Epoch 7/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6578 - accuracy: 0.5953 val_loss: 0.6541401743888855\n",
      "341/341 [==============================] - 10s 30ms/step - loss: 0.6578 - accuracy: 0.5953\n",
      "Epoch 8/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6395 - accuracy: 0.6166 val_loss: 0.6292906403541565\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.6395 - accuracy: 0.6166\n",
      "Epoch 9/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.6367 val_loss: 0.6055310368537903\n",
      "341/341 [==============================] - 10s 30ms/step - loss: 0.6188 - accuracy: 0.6367\n",
      "Epoch 10/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.6660 val_loss: 0.5908081531524658\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.5909 - accuracy: 0.6660\n",
      "Epoch 11/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.6870 val_loss: 0.5683087110519409\n",
      "341/341 [==============================] - 10s 30ms/step - loss: 0.5620 - accuracy: 0.6870\n",
      "Epoch 12/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.7119 val_loss: 0.5584611892700195\n",
      "341/341 [==============================] - 10s 30ms/step - loss: 0.5300 - accuracy: 0.7119\n",
      "Epoch 13/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.7307 val_loss: 0.54625403881073\n",
      "341/341 [==============================] - 10s 30ms/step - loss: 0.5013 - accuracy: 0.7307\n",
      "Epoch 14/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.7505 val_loss: 0.5332814455032349\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.4723 - accuracy: 0.7505\n",
      "Epoch 15/15\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.7632 val_loss: 0.5135179162025452\n",
      "341/341 [==============================] - 10s 30ms/step - loss: 0.4470 - accuracy: 0.7632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba1606f0b8>"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt=Adam(0.003)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "\n",
    "print('Training...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=15, callbacks=[TestCallback((X_val,y_val))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "uYRObiuTATfh",
    "outputId": "6db028d6-ad99-42c8-c046-0ac36c617030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 150, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 5,235,010\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AKYc3SFhjYS8",
    "outputId": "9440b075-0700-4de2-a336-7b77a56af2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 3s 9ms/step - loss: 0.6370 - accuracy: 0.6882\n",
      "Accuracy: 0.6881698966026306\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0DBhvcskhwnL",
    "outputId": "f64447eb-dc31-42d2-c171-ca9216b1fb6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6784722222222223\n",
      "Recall: 0.7603112840466926\n",
      "Confusion Matrix \n",
      " [[2898 1852]\n",
      " [1232 3908]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,confusion_matrix\n",
    "ypred = model.predict_classes(X_test)\n",
    "y_tes_classes = np.argmax(y_test,axis=1)\n",
    "print(\"Precision:\", precision_score(y_tes_classes,ypred))\n",
    "print(\"Recall:\", recall_score(y_tes_classes,ypred))\n",
    "print(\"Confusion Matrix \\n\",confusion_matrix(y_tes_classes,ypred))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DEMO_GLOVE_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
