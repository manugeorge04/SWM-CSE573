{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "colab_type": "code",
        "id": "Oc4Gw4fT9sjI",
        "outputId": "1d285d14-1ad0-4d9a-c0fb-64fef3cfb1fb"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#import ktrain\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#from ktrain import text\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "#import ktrain\n",
        "#from ktrain import text\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.layers import Dense, Flatten, Input, LSTM, Conv1D, GRU, Dropout\n",
        "from tensorflow.python.keras.layers.embeddings import Embedding\n",
        "from tensorflow.python.keras.preprocessing.text import one_hot\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "colab_type": "code",
        "id": "ekjgxoLe9wga",
        "outputId": "9b7d362b-a0c2-41a2-d678-2ab3ae742d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "['.ipynb_checkpoints', 'articlescraper.py', 'metadata.csv', 'news_join.ipynb', 'preprocess.ipynb', 'stockdata', 'tweetdata', 'unzip_data.py', 'model_bertBCE.h5', 'model_bert.ckpt.data-00001-of-00002', 'model_bert.ckpt.data-00000-of-00002', 'model_bert.ckpt.index', 'checkpoint', 'bert_Sentence_training.png']\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c468f0d77c74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/SWM-stock-prediction/code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/SWM-stock-prediction/code'"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive/')\n",
        "print(os.listdir('./'))\n",
        "os.chdir('drive/My Drive/SWM-stock-prediction/code')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tNYR0Hu3V8tw"
      },
      "outputs": [],
      "source": [
        "def split_document_to_chunks(row, fName, chunkLength):\n",
        "    chunkText = row[fName]\n",
        "    if(chunkText!='[]'):\n",
        "        label = row['label']\n",
        "        lines = sent_tokenize(chunkText)\n",
        "        result = []\n",
        "        for i in range(0,len(lines), chunkLength):\n",
        "            if(i+chunkLength < len(lines)):\n",
        "                chunk = ''.join(lines[i:i+chunkLength])\n",
        "            else:\n",
        "                chunk = ''.join(lines[i:len(lines)])\n",
        "            result.append((chunk,label))\n",
        "        return result\n",
        "    \n",
        "def prepare_data_set(dataSet):\n",
        "    chunkTxtLabel = dataSet.apply(split_document_to_chunks, args=('filteredtext_amzn', 3), axis=1)\n",
        "    X=[]\n",
        "    y=[]\n",
        "    for c in chunkTxtLabel:\n",
        "        if(c is not None):\n",
        "            for chunk in c:\n",
        "                X.append(chunk[0])\n",
        "                y.append(chunk[1])\n",
        "    return X,y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mXWEaWg4QthH"
      },
      "source": [
        "**Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WbZIXICnLzJC"
      },
      "outputs": [],
      "source": [
        "XTrain = np.load('../Data/X_train_bert.npy')\n",
        "XTest = np.load('../Data/X_test_bert.npy')\n",
        "trainLabels = np.load('../Data/label_train_bert.npy')\n",
        "testLabels = np.load('../Data/label_test_bert.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IBjAvHv2RUX4"
      },
      "source": [
        "**Train and test shapes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "KH3bO1exRZfd",
        "outputId": "b5483a9d-7c60-4045-c611-8809ec0a0fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(59278, 768)\n",
            "(13349, 768)\n"
          ]
        }
      ],
      "source": [
        "print(XTrain.shape)\n",
        "print(XTest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "46X51Fd4Q0Wj"
      },
      "source": [
        "**Loading the trained BERT Sentence embedding + DNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dsr9MqhZQlWs"
      },
      "outputs": [],
      "source": [
        "model = load_model('../Data/bert_sentence_NN.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G2orhYOORHpY"
      },
      "source": [
        "**DNN Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "colab_type": "code",
        "id": "W16ENr_MRGgD",
        "outputId": "523e957e-b414-45cf-c4d7-cbdc3c4d5a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_23 (Dense)             (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 591,361\n",
            "Trainable params: 591,361\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "8YM2Fp2ARN9w",
        "outputId": "873c4307-4394-4b11-dc45-d9a481e669fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "418/418 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6383\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.6531215310096741, 0.6383249759674072]"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(XTest, testLabels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "colab_type": "code",
        "id": "K2LVR8s3jV6w",
        "outputId": "69fb2557-2967-4b06-ee90-d032541ec167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-20-4c6eceff93e4>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "[[4130 2722]\n",
            " [2106 4391]]\n",
            "0.6173203992689442\n",
            "0.675850392488841\n",
            "0.6392970584744264\n"
          ]
        }
      ],
      "source": [
        "ypred = model.predict_classes(XTest)\n",
        "print(confusion_matrix(testLabels, ypred))\n",
        "print(precision_score(testLabels, ypred))\n",
        "print(recall_score(testLabels, ypred))\n",
        "print(roc_auc_score(testLabels, ypred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ofR5qDhekegw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "BERT_sentence_Evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
