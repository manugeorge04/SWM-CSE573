{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document_to_chunks(row, field_name, chunk_len):\n",
    "    text = row[field_name]\n",
    "    if(text!='[]'):\n",
    "        label = row['label']\n",
    "        sentences = sent_tokenize(text)\n",
    "        output = []\n",
    "        for i in range(0,len(sentences), chunk_len):\n",
    "            if(i+chunk_len < len(sentences)):\n",
    "                chunk = ''.join(sentences[i:i+chunk_len])\n",
    "            else:\n",
    "                chunk = ''.join(sentences[i:len(sentences)])\n",
    "            output.append((chunk,label))\n",
    "        return output\n",
    "def prepare_data_set(data_set,field_name, chunk_size=3):\n",
    "    chunked_text_labels = data_set.apply(split_document_to_chunks, args=(field_name, chunk_size), axis=1)\n",
    "    X=[]\n",
    "    y=[]\n",
    "    #print(\"Size before chunking: \", len(chunked_text_labels))\n",
    "    for chunks in chunked_text_labels:\n",
    "        if(chunks is not None):\n",
    "            for chunk in chunks:\n",
    "                X.append(chunk[0])\n",
    "                y.append(chunk[1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('../Data/final_dataset_joined_aapl240_onlyMentions.csv')\n",
    "data_set.loc[data_set['label']==-1,'label'] = 0\n",
    "train_df = data_set[data_set.stock_time <= \"2018-12-01 00:00:00\"]\n",
    "test_df = data_set[data_set.stock_time > \"2018-12-01 00:00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  59278 59278\n",
      "Test Shape:  13349 13349\n"
     ]
    }
   ],
   "source": [
    "X_train_text,y_train_labels = prepare_data_set(train_df,'filteredtext_aapl')\n",
    "X_test_text,y_test_labels = prepare_data_set(test_df,'filteredtext_aapl')\n",
    "print(\"Train Shape: \",len(X_train_text),len(y_train_labels))\n",
    "print(\"Test Shape: \",len(X_test_text),len(y_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, Bidirectional,LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 132808 unique tokens.\n",
      "Shape of data tensor: (72627, 100)\n",
      "Shape of label tensor: (72627, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "X_total = X_train_text+ X_test_text\n",
    "y_total = y_train_labels + y_test_labels\n",
    "\n",
    "print('Processing text dataset')\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_total)\n",
    "sequences = tokenizer.texts_to_sequences(X_total)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(y_total))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data tensor: (59278, 100)\n",
      "Shape of train label tensor: (59278, 2)\n",
      "Shape of test data tensor: (13349, 100)\n",
      "Shape of test label tensor: (13349, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:len(X_train_text)]\n",
    "y_train = labels[:len(y_train_labels)]\n",
    "X_test = data[len(X_train_text):]\n",
    "y_test = labels[len(y_train_labels):]\n",
    "print('Shape of train data tensor:', X_train.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "print('Shape of test data tensor:', X_test.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 59278 samples, validate on 13349 samples\n",
      "Epoch 1/50\n",
      "59278/59278 [==============================] - 297s 5ms/sample - loss: 0.6939 - accuracy: 0.5205 - val_loss: 0.6971 - val_accuracy: 0.4984\n",
      "Epoch 2/50\n",
      "59278/59278 [==============================] - 268s 5ms/sample - loss: 0.6829 - accuracy: 0.5443 - val_loss: 0.7044 - val_accuracy: 0.4884\n",
      "Epoch 3/50\n",
      "59264/59278 [============================>.] - ETA: 0s - loss: 0.6688 - accuracy: 0.5684"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "adam = Adam(0.01)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          validation_data=[X_test, y_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
